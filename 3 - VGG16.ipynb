{"cells":[{"cell_type":"markdown","metadata":{"id":"MTYxkVD5XtZT"},"source":["# Preliminary Operations\n","The same imports and auxiliary code is defined below:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"afMchSgGqcLE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703882335148,"user_tz":-60,"elapsed":16033,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"}},"outputId":"cb9bb379-140b-4bf8-fbf8-1ca99acd013f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","import numpy as np\n","import random as rn\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.preprocessing import label_binarize\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"iYX7AeYfr1WM","executionInfo":{"status":"ok","timestamp":1703882641155,"user_tz":-60,"elapsed":266,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"}}},"outputs":[],"source":["#ROOT_DIR = \"/content/drive/Shareddrives/Giaquinta_Pasqualetti/\"\n","ROOT_DIR = \"/content/sample_data/content/drive/Shareddrives/Giaquinta_Pasqualetti/\"\n","\n","IMAGES_DIR = os.path.join(ROOT_DIR, \"Data\")\n","TRAIN_DIR = os.path.join(IMAGES_DIR, \"Train\")\n","TEST_DIR = os.path.join(IMAGES_DIR, \"Test\")\n","\n","IMAGE_DIM = 64\n","RAN_SEED = 10024062\n","\n","VALIDATION_SPLIT = 0.1\n","TRAIN_SPLIT = 0.2"]},{"cell_type":"markdown","source":["NOTE: Interactions with Google Drive are commented out. For our own testing, we used files stored locally in the session to **significantly** speed up the computation. To run this notebook using files stored in Google Drive, uncomment the lines that are currently commented and comment out the lines that are using local files."],"metadata":{"id":"6Vo4mG1NYjRW"}},{"cell_type":"code","execution_count":26,"metadata":{"id":"Xn-3oyB7ti0E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703883144136,"user_tz":-60,"elapsed":418652,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"}},"outputId":"9a6245e2-fe0d-43e5-fe41-7a1985173bba"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/drive/Shareddrives/Giaquinta_Pasqualetti/Data': No such file or directory\n","replace /content/drive/Shareddrives/DeepLearning/Data/Test/Forest/Forest_520.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n","error:  invalid response [a]\n","replace /content/drive/Shareddrives/DeepLearning/Data/Test/Forest/Forest_520.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"]}],"source":["! rm -r /content/drive/Shareddrives/Giaquinta_Pasqualetti/Data\n","! unzip -q /content/drive/Shareddrives/Giaquinta_Pasqualetti/Data.zip -d /\n","\n","#! rm -r /content/sample_data/*\n","#! unzip -q /content/Data.zip -d /content/sample_data/"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ESLdEHS8t4Zc","executionInfo":{"status":"ok","timestamp":1703883150724,"user_tz":-60,"elapsed":330,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"}}},"outputs":[],"source":["# Set all the seeds, to create datasets and Tensors from the same starting point\n","def set_seed():\n","    os.environ[\"PYTHONHASHSEED\"]=\"0\"\n","    np.random.seed(RAN_SEED)\n","    rn.seed(RAN_SEED)\n","    tf.random.set_seed(RAN_SEED)\n","\n","# Returns Train, Val and Test sets\n","# BATCH_SIZE is user-given input, VALIDATION_SPLIT is 0.1\n","def load_datasets(BATCH_SIZE):\n","    set_seed()\n","\n","    train = tf.keras.preprocessing.image_dataset_from_directory(\n","        TRAIN_DIR, labels='inferred', label_mode='categorical', class_names=None,\n","        color_mode='rgb', batch_size=BATCH_SIZE, shuffle=True, seed=RAN_SEED,\n","        validation_split=VALIDATION_SPLIT, subset='training', follow_links=False,\n","        image_size=(64,64)\n","    )\n","\n","    val = tf.keras.preprocessing.image_dataset_from_directory(\n","        TRAIN_DIR, labels='inferred', label_mode='categorical', class_names=None,\n","        color_mode='rgb', batch_size=BATCH_SIZE, shuffle=True, seed=RAN_SEED,\n","        validation_split=VALIDATION_SPLIT, subset='validation', follow_links=False,\n","        image_size=(64,64)\n","    )\n","\n","    test = tf.keras.preprocessing.image_dataset_from_directory(\n","        TEST_DIR, labels='inferred', label_mode='categorical',\n","        class_names=None, color_mode='rgb', batch_size=BATCH_SIZE, shuffle=True,\n","        seed=RAN_SEED, follow_links=False, image_size=(64,64)\n","    )\n","\n","    return train, val, test\n","\n","# Returns some details about trained model\n","def train_performance(history):\n","\n","  acc = history.history['accuracy']\n","  val_acc = history.history['val_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  epochs = range(len(acc))\n","\n","  plt.plot(epochs, acc, 'bo', label='Training accuracy')\n","  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","\n","  plt.figure()\n","\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","\n","  plt.show()\n","\n","# Test\n","def show_res(model, test):\n","  data, labels = [], []\n","\n","  for data_batch, labels_batch in test:\n","    data.append(data_batch)\n","    labels.append(labels_batch)\n","\n","  data = tf.concat(data, axis=0)\n","  labels = tf.concat(labels, axis=0)\n","\n","  y_score = model.predict(data)\n","\n","  y_pred = np.rint(y_score)\n","\n","  y_true = np.asarray(labels)\n","\n","  y_pred = np.argmax(y_score, axis=1)\n","  y_test = np.argmax(y_true, axis=1)\n","\n","  print(\"Classification report: \")\n","  print(metrics.classification_report(y_test,y_pred ,digits = 4))\n","\n","\n","  cm = metrics.confusion_matrix(y_test, y_pred)\n","  disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n","  disp.plot()\n","  plt.show()\n","\n","\n","# Function to plot multi-class ROC curves\n","def plot_multiclass_roc(model, test_data):\n","    test_images, test_labels = [], []\n","\n","    for data_batch, labels_batch in test_data:\n","        test_images.append(data_batch)\n","        test_labels.append(labels_batch)\n","\n","    test_images = np.concatenate(test_images, axis=0)\n","    test_labels = np.concatenate(test_labels, axis=0)\n","\n","    # Preprocess images\n","    test_images = preprocess_input(test_images)\n","\n","    # Get predicted scores for all classes\n","    y_scores = model.predict(test_images)\n","\n","    # Binarize labels\n","    y_true = label_binarize(np.argmax(test_labels, axis=1), classes=range(10))\n","\n","    # Compute ROC curve and AUC for each class\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(10):\n","        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_scores[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    # Plot all ROC curves in a single plot\n","    plt.figure(figsize=(10, 8))\n","    for i in range(10):\n","        plt.plot(fpr[i], tpr[i], label='Class %d (AUC = %0.2f)' % (i, roc_auc[i]))\n","\n","    plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Multi-Class ROC Curves')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","\n","def model_summary(model):\n","  from keras.utils.vis_utils import plot_model\n","\n","  model.summary()\n","  plot_model(model, show_shapes=True, show_layer_names=True)\n"]},{"cell_type":"markdown","metadata":{"id":"Fc3KsDECpnoW"},"source":["# VGG16 with Feature Extraction\n","Importing the **VGG16** Keras model. VGG16 is a deep neural network architecture known for its simplicity and effectiveness in image classification. It consists of 16 layers, mostly using 3x3 convolutional filters and 2x2 max-pooling layers. VGG16 captures features at different scales, making it useful for tasks like recognizing objects in images. Its pre-trained weights enable quick adaptation to new tasks, making it a popular choice for transfer learning.\n","\n","Using VGG16 for satellite image land use classification is beneficial due to its hierarchical feature extraction, pre-trained weights, transfer learning potential, ability to handle large input sizes, and suitability for diverse landscapes. It offers good generalization and customization while aiding visual interpretation of features."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":887,"status":"ok","timestamp":1703883155530,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"},"user_tz":-60},"id":"LPrkXPqX4l9K","outputId":"a480f179-9e52-4690-89bf-7266bfe1329c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"vgg16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 64, 64, 3)]       0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n","                                                                 \n","=================================================================\n","Total params: 14714688 (56.13 MB)\n","Trainable params: 14714688 (56.13 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras.applications import VGG16\n","\n","conv_base = keras.applications.vgg16.VGG16(\n","    weights=\"imagenet\",\n","    include_top=False,\n","    input_shape=(64, 64, 3))\n","\n","conv_base.summary()"]},{"cell_type":"markdown","metadata":{"id":"eauK3_K02K4V"},"source":["Layer freezing is needed to fix the weights of certain neural network layers during training, commonly for transfer learning, to retain learned features and prevent them from being changed."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1703883159294,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"},"user_tz":-60},"id":"-wzfniwoBLgW","outputId":"a7f3cd53-e38f-42fa-ff32-ffa3305748f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of trainable weights:  14714688\n","Number of trainable weights after freezing the convolutional base: 0\n"]}],"source":["print('Number of trainable weights: ', sum(np.prod(x.shape) for x in conv_base.trainable_weights))\n","conv_base.trainable = False\n","print('Number of trainable weights after freezing the convolutional base:', sum(np.prod(x.shape) for x in conv_base.trainable_weights))"]},{"cell_type":"markdown","metadata":{"id":"4WTv-OeaqOPe"},"source":["## **First try:** 256 neurons with adam optimizer\n","The conv_base initiates the feature extraction phase. As our parameters we decided to set the number of neurons in the dense layer to 256 and the number of neurons in the dense layer to 256, without any L2 regularization, dropout or data augmentation, which will be later added if necessary. The chosen optimizer is Adam."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1703883162159,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"},"user_tz":-60},"id":"SBb9YHsGBZ1w","outputId":"cf59b64b-b98d-4053-f247-b8f631a0f8f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_8 (InputLayer)        [(None, 64, 64, 3)]       0         \n","                                                                 \n"," tf.__operators__.getitem_3  (None, 64, 64, 3)         0         \n","  (SlicingOpLambda)                                              \n","                                                                 \n"," tf.nn.bias_add_3 (TFOpLamb  (None, 64, 64, 3)         0         \n"," da)                                                             \n","                                                                 \n"," vgg16 (Functional)          (None, 2, 2, 512)         14714688  \n","                                                                 \n"," flatten_3 (Flatten)         (None, 2048)              0         \n","                                                                 \n"," dense_6 (Dense)             (None, 256)               524544    \n","                                                                 \n"," dense_7 (Dense)             (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 15241802 (58.14 MB)\n","Trainable params: 527114 (2.01 MB)\n","Non-trainable params: 14714688 (56.13 MB)\n","_________________________________________________________________\n"]}],"source":["inputs = keras.Input(shape=(64, 64, 3))\n","x = inputs\n","x = keras.applications.vgg16.preprocess_input(x)\n","x = conv_base(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(256)(x)\n","outputs = layers.Dense(10, activation=\"softmax\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(loss=\"categorical_crossentropy\",\n","              optimizer=\"adam\",\n","              metrics=[\"accuracy\"])\n","model.summary()"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"3_NWlabNCDMi","executionInfo":{"status":"ok","timestamp":1703883164766,"user_tz":-60,"elapsed":2,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"}}},"outputs":[],"source":["callbacks_list = [\n","    keras.callbacks.EarlyStopping(\n","        monitor='val_accuracy',\n","        patience=5,\n","    ),\n","    keras.callbacks.ModelCheckpoint(\n","        #'/content/drive/Shareddrives/Giaquinta_Pasqualetti/Models/vgg16_simple.h5',\n","        '/content/vgg16_simple.h5',\n","        monitor='val_loss',\n","        mode='min',\n","        save_best_only=True,\n","    )\n","]"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":304,"status":"error","timestamp":1703883166908,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"},"user_tz":-60},"id":"BiihdgzICQK3","outputId":"f1e72d7e-8b7c-4b30-c358-0b1764be538f"},"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-d0c721660b16>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshow_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_multiclass_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-d68ac58d5365>\u001b[0m in \u001b[0;36mload_datasets\u001b[0;34m(BATCH_SIZE)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     train = tf.keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inferred'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAN_SEED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /content/sample_data/content/drive/Shareddrives/Giaquinta_Pasqualetti/Data/Train"]}],"source":["train, val, test = load_datasets(BATCH_SIZE = 64)\n","res = model.fit(train, epochs=15, validation_data=val, callbacks = callbacks_list)\n","train_performance(res)\n","show_res(model, test)\n","plot_multiclass_roc(model, test)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"IBNBZlE2Mh2_"}},{"cell_type":"markdown","metadata":{"id":"NvMPhBomjtFm"},"source":["## **Second try:** addressing the overfitting\n","The results indicate that the validation accuracy is lower than the training accuracy, and the validation loss is higher than the training loss by a significant margin. It is possible that the accuracy might not improve significantly or not improve at all. However, let's attempt to reduce overfitting so that the classification model is sufficiently generalized to consistently deliver this level of performance. To begin with, we will implement dropout and add a data augmentation layer.\n","\n","Let's repeat the experiment:"]},{"cell_type":"code","source":["data_augmentation = keras.Sequential([\n","    layers.RandomFlip(\"horizontal\"),\n","    layers.RandomRotation(0.1),\n","    layers.RandomZoom(0.2),\n","])\n","\n","inputs = keras.Input(shape=(64, 64, 3))\n","x = data_augmentation(inputs)\n","x = inputs\n","x = keras.applications.vgg16.preprocess_input(x)\n","x = conv_base(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(256)(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(10, activation=\"softmax\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(loss=\"categorical_crossentropy\",\n","              optimizer=\"adam\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks_list = [\n","    keras.callbacks.EarlyStopping(\n","        monitor='val_accuracy',\n","        patience=5,\n","    ),\n","    keras.callbacks.ModelCheckpoint(\n","        #'/content/drive/Shareddrives/Giaquinta_Pasqualetti/Models/vgg16_dropout_augment.h5',\n","        '/content/vgg16_dropout_augment.h5',\n","        monitor='val_loss',\n","        mode='min',\n","        save_best_only=True,\n","    )\n","]\n","\n","train, val, test = load_datasets(BATCH_SIZE = 64)\n","res = model.fit(train, epochs=15, validation_data=val, callbacks = callbacks_list)\n","train_performance(res)\n","show_res(model, test)\n","plot_multiclass_roc(model, test)"],"metadata":{"id":"PR497HBkEkar","executionInfo":{"status":"aborted","timestamp":1703882544540,"user_tz":-60,"elapsed":9,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dlnu6cUUKqW2"},"source":["## **Third try:** Data regularization\n","Presently, an instance of overfitting can still be observed. The magnitude of the problem has decreased thanks to the dropout and data augmentation layer. Let's attempt to address this once and for all by also incorporating L2 regularization into the dense layer."]},{"cell_type":"code","source":["from keras.regularizers import l2\n","\n","data_augmentation = keras.Sequential([\n","    layers.RandomFlip(\"horizontal\"),\n","    layers.RandomRotation(0.1),\n","    layers.RandomZoom(0.2),\n","])\n","\n","inputs = keras.Input(shape=(64, 64, 3))\n","x = data_augmentation(inputs)\n","x = inputs\n","x = keras.applications.vgg16.preprocess_input(x)\n","x = conv_base(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(256, kernel_regularizer=l2(0.01))(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(10, activation=\"softmax\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(loss=\"categorical_crossentropy\",\n","              optimizer=\"adam\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks_list = [\n","    keras.callbacks.EarlyStopping(\n","        monitor='val_accuracy',\n","        patience=5,\n","    ),\n","    keras.callbacks.ModelCheckpoint(\n","        #'/content/drive/Shareddrives/Giaquinta_Pasqualetti/Models/vgg16_final.h5',\n","        '/content/vgg16_final.h5',\n","        monitor='val_loss',\n","        mode='min',\n","        save_best_only=True,\n","    )\n","]\n","\n","train, val, test = load_datasets(BATCH_SIZE = 64)\n","res = model.fit(train, epochs=15, validation_data=val, callbacks = callbacks_list)\n","train_performance(res)\n","show_res(model, test)\n","plot_multiclass_roc(model, test)"],"metadata":{"id":"RNIiWAAxE0vI","executionInfo":{"status":"aborted","timestamp":1703882544540,"user_tz":-60,"elapsed":9,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPPJedyYTVTY"},"source":["# Fine Tuning\n","While this might not be the best idea (fine-tuning assumes that a model has been trained on similar data and with similar classes), we decided to try this approach anyway because we lack precise knowledge of the VGG16 model and because of the limited size of our dataset; we cannot definitively state that this will lead to worse results.\n","\n","The model is composed of 19 layers as we can see below:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1703882544540,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"},"user_tz":-60},"id":"R9MpWLHMTO7L"},"outputs":[],"source":["#model = tf.keras.models.load_model('/content/drive/Shareddrives/Giaquinta_Pasqualetti/Models/vgg16_final.h5')\n","model = tf.keras.models.load_model('/content/vgg16_final.h5')\n","\n","for i, layer in enumerate(model.get_layer('vgg16').layers):\n","    print(i, layer.name, layer.trainable)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1703882544540,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"},"user_tz":-60},"id":"2tg29623O-sF"},"outputs":[],"source":["conv_base.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1703882544541,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"},"user_tz":-60},"id":"hGlV7n_5OYwP"},"outputs":[],"source":["# Unfreeze every block\n","model.get_layer('vgg16').trainable = True\n","\n","# Freeze every block except the last one\n","for layer in model.get_layer('vgg16').layers[0:15]:\n","    layer.trainable = False\n","# Make sure you have frozen the correct layers\n","for i, layer in enumerate(model.get_layer('vgg16').layers):\n","    print(i, layer.name, layer.trainable)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1703882544541,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"},"user_tz":-60},"id":"tslAxeqbPA9U"},"outputs":[],"source":["model.get_layer('vgg16').summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmqM8Qu7P_UA","executionInfo":{"status":"aborted","timestamp":1703882544541,"user_tz":-60,"elapsed":8,"user":{"displayName":"DANIELE GIAQUINTA","userId":"08054233077691606074"}}},"outputs":[],"source":["callbacks_list = [\n","    keras.callbacks.EarlyStopping(\n","        monitor='val_accuracy',\n","        patience=5,\n","    ),\n","    keras.callbacks.ModelCheckpoint(\n","        #'/content/drive/Shareddrives/Giaquinta_Pasqualetti/Models/vgg16_finetuned.h5',\n","        '/content/vgg16_finetuned.h5',\n","        monitor='val_loss',\n","        mode='min',\n","        save_best_only=True,\n","    )\n","]\n","\n","train, val, test = load_datasets(BATCH_SIZE = 64)\n","res = model.fit(train, epochs=15, validation_data=val, callbacks = callbacks_list)\n","train_performance(res)\n","show_res(model, test)\n","plot_multiclass_roc(model, test)"]},{"cell_type":"markdown","source":["Surely an unstable and unreliable model, fine tuning is not a good idea"],"metadata":{"id":"BlasFMvcYRoV"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1UYNkTjedKN1LxOAXz2fJIeyXOBhmlaWG","timestamp":1691590726612}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}